ğŸ§  Prompt Layer
Reusable Prompt Templates, Versioning & Observability for LLM Systems








A lightweight PromptOps layer to manage, version, validate, and monitor reusable LLM prompt templates â€” built using Python, Flask, YAML, Jinja2, and JSON Schema.

Treat prompts like code: versioned, validated, observable, and reusable.

ğŸš€ Project Description (GitHub Summary)

Prompt Layer provides an API-driven abstraction over LLM prompts, enabling teams to:

Store prompts as structured YAML files

Render prompts dynamically with Jinja2

Enforce output schemas using JSON Schema

Track latency, versions, and usage metrics

Expose observability endpoints for production systems

This project is ideal for LLM applications, AI agents, RAG systems, and PromptOps workflows where prompt quality, consistency, and monitoring matter.

âœ¨ Key Features
âœ… Prompt Templates as Code

Prompts stored in prompts/*.yaml

Each prompt includes:

prompt_id

semantic version

description

Jinja2 template

example inputs

expected output schema

âœ… Versioning & Metadata

Semantic version comparison using packaging.version

Fetch latest or specific prompt versions

Metadata-first design for future evaluation & A/B testing

âœ… Dynamic Prompt Rendering

Render prompts dynamically through REST APIs

Jinja2-based templating for flexible input injection

âœ… Usage Logging (Prompt Telemetry)

Append-only structured logs in prompt_usage.jsonl

Logs include:

timestamp

prompt id & version

latency

model info

custom metadata

âœ… Schema Validation

Enforces JSON Schema validation on model outputs

Fails fast on malformed or hallucinated outputs

âœ… Metrics & Observability

Auto-aggregated metrics:

total usage count

average latency

last seen timestamp

latest version

Available via /metrics

Optional persistence (prompt_metrics.json)

âœ… Health & Debug Endpoints

/health â€” service heartbeat

/last-usage â€” last prompt execution snapshot

ğŸ§© Project Structure
prompt-layer/
â”‚
â”œâ”€â”€ prompts/
â”‚   â”œâ”€â”€ summarization.yaml
â”‚   â””â”€â”€ classification.yaml
â”‚
â”œâ”€â”€ src/
â”‚   â””â”€â”€ prompt_manager.py   # Core API, rendering, logging, metrics
â”‚
â”œâ”€â”€ prompt_usage.jsonl      # Append-only usage logs
â”œâ”€â”€ prompt_metrics.json     # Persisted metrics (auto-generated)
â””â”€â”€ README.md

ğŸ§  Example Prompt Definition
prompt_id: summarization_short
version: "1.0.0"
description: Create a short (max 50 words) summary for provided text.
template: |
  Summarize the following text in one paragraph, maximum 50 words:
  {{ text }}
example_inputs:
  - text: "Long article text..."
expected_output_schema:
  type: object
  properties:
    summary:
      type: string
output_instructions: >
  Return a JSON object: { "summary": "<50-word summary>" }

âš™ï¸ Setup & Run
1ï¸âƒ£ Clone the Repo
git clone https://github.com/madhuram99/prompt-layer.git
cd prompt-layer

2ï¸âƒ£ Install Dependencies
pip install flask pyyaml jinja2 jsonschema packaging

3ï¸âƒ£ Run the API
python src/prompt_manager.py


ğŸŸ¢ Server runs at:
http://127.0.0.1:5000

ğŸ§ª API Usage Examples
ğŸ”¹ Health Check
curl http://127.0.0.1:5000/health

ğŸ”¹ Get Prompt Definition
curl http://127.0.0.1:5000/prompt/summarization_short

ğŸ”¹ Render Prompt
curl -X POST http://127.0.0.1:5000/prompt/summarization_short/render \
  -H "Content-Type: application/json" \
  -d '{"version":"1.0.0","inputs":{"text":"OpenAI released a new model."}}'

ğŸ”¹ Log Usage
curl -X POST http://127.0.0.1:5000/prompt/summarization_short/log \
  -H "Content-Type: application/json" \
  -d '{
    "version":"1.0.0",
    "input":{"text":"Long article"},
    "response":{"summary":"Short summary"},
    "latency_ms":243.5,
    "metadata":{"model":"gpt-5"}
  }'

ğŸ”¹ Metrics Endpoint
curl http://127.0.0.1:5000/metrics
